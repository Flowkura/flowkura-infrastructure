# üöÄ Flowkura Infrastructure

Infrastructure compl√®te pour d√©ployer RAGFlow avec Qwen3-8B et BGE-M3 sur GPU.

## üìã Pr√©requis

- Docker et Docker Compose install√©s
- GPU NVIDIA avec drivers install√©s
- NVIDIA Container Toolkit configur√©
- Token Hugging Face (pour t√©l√©charger les mod√®les)

## üõ†Ô∏è Installation Rapide

### 1. Cloner le repository

```bash
git clone https://github.com/Flowkura/flowkura-infrastructure.git
cd flowkura-infrastructure
```

### 2. Configurer les variables d'environnement

```bash
cp .env.example .env
nano .env  # Remplacer your_huggingface_token_here par votre token HF
```

### 3. Cr√©er la structure de dossiers

```bash
mkdir -p ragflow/volumes/{ragflow,nginx}
mkdir -p ollama
```

### 4. Lancer les services

```bash
docker-compose up -d
```

### 5. T√©l√©charger le mod√®le d'embedding

```bash
docker exec -it ollama ollama pull bge-m3
```

## üì¶ Services D√©ploy√©s

### RAGFlow (Port 9380, 80, 443)
- **Image**: `infiniflow/ragflow:v0.15.0`
- **Fonction**: Interface principale et moteur RAG
- **Acc√®s**: http://localhost:9380

### Ollama (Port 11434)
- **Image**: `ollama/ollama:latest`
- **Mod√®le**: BGE-M3 (embedding multilingue fran√ßais)
- **Fonction**: G√©n√©ration d'embeddings pour la recherche s√©mantique

### SGLang (Port 8000)
- **Image**: `lmsysorg/sglang:latest`
- **Mod√®le**: Qwen3-8B
- **Fonction**: Serveur LLM pour la g√©n√©ration de texte

## ‚öôÔ∏è Configuration dans RAGFlow

### 1. Ajouter Ollama (Embedding)

Dans RAGFlow > Settings > Model Providers:

```
Type: Ollama
Base URL: http://ollama:11434
Model: bge-m3
Type: embedding
```

### 2. Ajouter SGLang (LLM)

Dans RAGFlow > Settings > Model Providers:

```
Type: OpenAI-API-Compatible
Name: VLLM
Base URL: http://sglang:8000/v1
API Key: EMPTY
Model: Qwen3-8B
Type: chat
Max Tokens: 8192
```

### 3. Configurer vos Datasets

Pour chaque dataset:
1. Aller dans Knowledge Base > Votre Dataset > Settings
2. Embedding Model: `bge-m3@Ollama`
3. Chunk Method: `naive` (General)
4. Chunk Token Count: `512` (ou selon vos besoins)

## üîß Commandes Utiles

### V√©rifier les logs
```bash
docker-compose logs -f ragflow    # Logs RAGFlow
docker-compose logs -f ollama     # Logs Ollama
docker-compose logs -f sglang     # Logs SGLang
```

### Red√©marrer un service
```bash
docker-compose restart ragflow
docker-compose restart ollama
docker-compose restart sglang
```

### Arr√™ter tous les services
```bash
docker-compose down
```

### Supprimer tout (‚ö†Ô∏è ATTENTION: supprime les donn√©es)
```bash
docker-compose down -v
```

### V√©rifier le mod√®le Ollama
```bash
docker exec -it ollama ollama list
```

### Tester l'embedding Ollama
```bash
curl http://localhost:11434/api/embeddings \
  -d '{"model": "bge-m3", "prompt": "Bonjour le monde"}'
```

### Tester SGLang
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen3-8B",
    "messages": [{"role": "user", "content": "Bonjour!"}],
    "max_tokens": 100
  }'
```

## üìä Optimisations Appliqu√©es

### Base de donn√©es (Redis + MySQL + Elasticsearch)
- Configuration optimis√©e pour GPU
- Augmentation des buffers et cache
- Pooling optimis√©

### RAGFlow
- GPU activ√© (`CPUONLY=0`)
- Max tokens augment√© (8192)
- Registration d√©sactiv√©e

### SGLang
- `mem-fraction-static 0.85` : Utilisation optimale de la VRAM
- `trust-remote-code` : Support complet de Qwen3

## üîê S√©curit√©

- Le `.env` est dans `.gitignore` (ne jamais commit les tokens)
- Utilisez `.env.example` comme template
- Changez les ports si n√©cessaire pour votre infrastructure

## üêõ Troubleshooting

### RAGFlow ne d√©marre pas
```bash
docker-compose logs ragflow
# V√©rifier que le GPU est bien d√©tect√©
docker run --rm --gpus all nvidia/cuda:12.1.0-base-ubuntu22.04 nvidia-smi
```

### Ollama ne t√©l√©charge pas le mod√®le
```bash
# V√©rifier l'espace disque
df -h
# T√©l√©charger manuellement
docker exec -it ollama ollama pull bge-m3
```

### SGLang out of memory
```bash
# R√©duire mem-fraction-static dans docker-compose.yml
# De 0.85 √† 0.7 par exemple
```

## üìà Performance

- **Parsing**: ~500-1000 documents/heure (selon complexit√©)
- **Embedding**: ~100 chunks/seconde
- **G√©n√©ration**: ~20-30 tokens/seconde

## üÜò Support

- GitHub Issues: [https://github.com/Flowkura/flowkura-infrastructure/issues](https://github.com/Flowkura/flowkura-infrastructure/issues)
- Documentation RAGFlow: [https://ragflow.io/docs](https://ragflow.io/docs)

## üìù License

MIT
