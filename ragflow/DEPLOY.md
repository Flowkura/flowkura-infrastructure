# üöÄ Flowkura RAGFlow - Guide de D√©ploiement

## üìã Pr√©requis

- Ubuntu 22.04 LTS
- Docker & Docker Compose V2
- NVIDIA GPU avec drivers + nvidia-container-toolkit
- 32 GB RAM minimum
- 100 GB espace disque

## üîß Installation Initiale

### 1. Cloner le repository

```bash
cd /root
git clone https://github.com/Flowkura/flowkura-infrastructure.git
cd flowkura-infrastructure/ragflow/docker
```

### 2. Configurer les variables d'environnement

Copier le fichier `.env` et ajuster les valeurs :

```bash
cp .env .env.local
nano .env
```

Variables importantes :
- `HF_TOKEN` : Token Hugging Face
- `TIMEZONE` : Europe/Paris
- `RAGFLOW_IMAGE` : infiniflow/ragflow:latest (GPU version)

### 3. Premi√®re installation

```bash
# Lancer tous les services
docker compose -f docker-compose-gpu.yml up -d

# V√©rifier que tout tourne
docker compose -f docker-compose-gpu.yml ps

# Suivre les logs
docker compose -f docker-compose-gpu.yml logs -f
```

### 4. T√©l√©charger le mod√®le d'embedding dans Ollama

```bash
# Entrer dans le container Ollama
docker exec -it flowkura-ollama ollama pull bge-m3

# V√©rifier
docker exec -it flowkura-ollama ollama list
```

## üîÑ Op√©rations Courantes

### Red√©marrer tous les services

```bash
cd /root/flowkura-infrastructure/ragflow/docker
docker compose -f docker-compose-gpu.yml restart
```

### Red√©marrer un service sp√©cifique

```bash
# RAGFlow uniquement
docker compose -f docker-compose-gpu.yml restart ragflow

# SGLang LLM uniquement
docker compose -f docker-compose-gpu.yml restart sglang-qwen3

# Ollama uniquement
docker compose -f docker-compose-gpu.yml restart ollama
```

### Voir les logs

```bash
# Tous les services
docker compose -f docker-compose-gpu.yml logs -f

# Service sp√©cifique
docker compose -f docker-compose-gpu.yml logs -f ragflow
docker compose -f docker-compose-gpu.yml logs -f sglang-qwen3
docker compose -f docker-compose-gpu.yml logs -f ollama
```

### Arr√™ter les services

```bash
# Arr√™t complet
docker compose -f docker-compose-gpu.yml down

# Arr√™t avec suppression des volumes (‚ö†Ô∏è ATTENTION : perte de donn√©es)
docker compose -f docker-compose-gpu.yml down -v
```

## üéØ Architecture des Services

### Services principaux

1. **RAGFlow** (Port 9380, 8081)
   - Interface web principale
   - API REST
   - GPU : Partag√© avec tous les services

2. **MySQL** (Port 3306)
   - Base de donn√©es principale
   - Stockage des m√©tadonn√©es

3. **Elasticsearch** (Port 9200)
   - Index de recherche
   - Stockage des embeddings

4. **Redis** (Port 6379)
   - Cache
   - Queue de t√¢ches

5. **SGLang - Qwen3-8B** (Port 30000)
   - Mod√®le LLM principal
   - API compatible OpenAI
   - GPU : 70% VRAM

6. **Ollama - BGE-M3** (Port 11434)
   - Mod√®le d'embedding multilingue
   - Optimis√© pour le fran√ßais
   - GPU : Partag√©

### R√©partition GPU

- **SGLang (Qwen3-8B)** : ~70% VRAM (18-20 GB)
- **Ollama (BGE-M3)** : ~20% VRAM (4-5 GB)
- **RAGFlow parsers** : ~10% VRAM (2-3 GB)

## üîê Configuration RAGFlow

### 1. Premier acc√®s

- URL : `http://[IP_SERVEUR]:9380`
- Cr√©er un compte admin

### 2. Configurer les mod√®les

Dans RAGFlow > Settings > Model Providers :

#### LLM (SGLang - Qwen3-8B)
- **Type** : VLLM
- **Model Name** : Qwen/Qwen3-8B
- **Base URL** : http://sglang-qwen3:30000/v1
- **API Key** : (laisser vide)
- **Max Tokens** : 8192

#### Embedding (Ollama - BGE-M3)
- **Type** : Ollama
- **Model Name** : bge-m3
- **Base URL** : http://ollama:11434
- **API Key** : (laisser vide)

### 3. Cr√©er un Dataset

- Aller dans Datasets > Create
- Nom : Ex. "Fiches M√©tiers ONISEP"
- Embedding Model : **bge-m3@Ollama**
- Chunk Method : **naive**
- Parser Config :
  ```json
  {
    "chunk_token_num": 512,
    "delimiter": "\\n!?„ÄÇ",
    "layout_recognize": "DeepDOC",
    "raptor": {"use_raptor": false},
    "graphrag": {"use_graphrag": false}
  }
  ```

### 4. Upload et Parser des documents

```bash
# Depuis la machine locale
cd ~/Workspace/Flowkura/llm
python upload_to_ragflow.py
```

## üîç Monitoring & Debug

### V√©rifier la sant√© des services

```bash
# Status de tous les containers
docker compose -f docker-compose-gpu.yml ps

# Utilisation GPU
nvidia-smi

# Espace disque
df -h

# M√©moire RAM
free -h
```

### Logs importants

```bash
# RAGFlow logs
tail -f /root/flowkura-infrastructure/ragflow/docker/ragflow-logs/api.log

# MySQL logs
docker compose -f docker-compose-gpu.yml logs mysql | tail -100

# Elasticsearch logs
docker compose -f docker-compose-gpu.yml logs es01 | tail -100
```

### Probl√®mes courants

#### SGLang ne d√©marre pas
```bash
# V√©rifier la VRAM disponible
nvidia-smi

# R√©duire mem-fraction-static dans docker-compose-gpu.yml si n√©cessaire
```

#### Ollama model non trouv√©
```bash
# Re-t√©l√©charger le mod√®le
docker exec -it flowkura-ollama ollama pull bge-m3
```

#### RAGFlow : Connection Error aux models
```bash
# V√©rifier que les services sont sur le m√™me network
docker network inspect ragflow_ragflow

# Red√©marrer RAGFlow
docker compose -f docker-compose-gpu.yml restart ragflow
```

## üìä Optimisations Base de Donn√©es

Les optimisations suivantes sont d√©j√† configur√©es dans `.env` :

### MySQL
- `innodb_buffer_pool_size=8G` : Cache des donn√©es
- `innodb_log_file_size=1G` : Taille des logs
- `max_connections=500` : Connexions simultan√©es

### Elasticsearch
- Heap size : 4GB (ES_JAVA_OPTS)
- Indices shards : 1 shard par d√©faut

### Redis
- `maxmemory=4gb`
- `maxmemory-policy=allkeys-lru`

## üîÑ Mise √† Jour

```bash
cd /root/flowkura-infrastructure/ragflow/docker

# Pull derni√®res images
docker compose -f docker-compose-gpu.yml pull

# Red√©marrer
docker compose -f docker-compose-gpu.yml up -d

# V√©rifier
docker compose -f docker-compose-gpu.yml ps
```

## üÜò Support

- Documentation RAGFlow : https://ragflow.io/docs
- GitHub Issues : https://github.com/Flowkura/flowkura-infrastructure/issues

## üìù Changelog

### 2025-11-19
- ‚úÖ Optimisations MySQL, Elasticsearch, Redis
- ‚úÖ Migration de vLLM vers SGLang pour Qwen3-8B
- ‚úÖ Ajout Ollama pour BGE-M3 (embedding multilingue)
- ‚úÖ Configuration GPU optimis√©e
- ‚úÖ Documentation compl√®te

---

**Flowkura Team** - Powered by RAGFlow + SGLang + Ollama
